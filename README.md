# Retinal Vessel Segmentation Benchmark

A modular system for training and comparing U-Net/U-Lite variants on CHASE, DRIVE, STARE, and HRF datasets.

## ğŸ“ Repository Structure

```
SA-U-Lite_retinal_vessel_segmentation/
â”œâ”€â”€ benchmark.py                # Interactive benchmarking runner
â”œâ”€â”€ run_benchmark.py            # CLI for batch benchmarking
â”œâ”€â”€ plot_result.py              # CLI for visualizing quantitative results (curves/metrics)
â”œâ”€â”€ inference_plot.py           # CLI for qualitative segmentation comparison grid
â”œâ”€â”€ config.py                   # Datasets, model registry, training config
â”œâ”€â”€ data_loader.py              # Dataset loading utilities
â”œâ”€â”€ losses.py                   # Loss functions
â”œâ”€â”€ metrics.py                  # Evaluation metrics
â”œâ”€â”€ trainer.py                  # Training loop and logging
â”œâ”€â”€ models/                     # Model implementations
â”‚   â”œâ”€â”€ saulite.py
â”‚   â”œâ”€â”€ ... (other models)
â”‚   â””â”€â”€ w_net.py
â””â”€â”€ results/                    # Auto-generated experiment outputs
```

## ğŸš€ Quick Start

### Interactive Training

Run the interactive benchmarking script:

```powershell
python .\benchmark.py
```

You will be prompted to choose dataset(s), model(s), and epochs.

### Command-Line Benchmark (CLI)

Use `run_benchmark.py` to run experiments directly from the terminal:

```powershell
# Basic run
python .\run_benchmark.py --dataset CHASE --models SA-U-Lite --epochs 50

# Multiple models across datasets
python .\run_benchmark.py -d DRIVE -m SA-U-Lite,W-Lite,SDA-U-Lite -e 25

# List options
python .\run_benchmark.py --list-datasets
python .\run_benchmark.py --list-models
```

### Results Visualization (Quantitative)

Use `plot_result.py` to visualize training curves (loss/dice) and performance metrics bar charts:

```powershell
# Interactive mode
python .\plot_result.py

# Plot specific dataset/epoch
python .\plot_result.py --dataset CHASE --epochs 50_epochs
```

### Qualitative Comparison (Inference Grid)

Use `inference_plot.py` to generate a high-resolution visual grid comparing actual segmentation outputs. The tool creates a matrix where **Columns represent Datasets** and **Rows represent Models**, including the Original Image and Ground Truth for reference.

**Usage:**

```powershell
# Compare SA-U-Lite and U-Lite models on CHASE and DRIVE datasets (using 50 epoch weights)
python .\inference_plot.py --datasets CHASE,DRIVE --models SA-U-Lite,U-Lite --epoch 50

# Compare a single model across all four datasets
python .\inference_plot.py -d CHASE,DRIVE,STARE,HRF -m W-Net -e 25
```

**Outputs:**
Saves a high-resolution PNG file (e.g., `comparison_grid_50ep.png`) in the root directory displaying the segmentation masks.

Exemple of the output :

<p align="center">
  <img src="doc/plot_mod.png" alt="Comparaison grid 100 epoch" width="800">
</p>


## ğŸ“Š Available Models

This benchmark includes various U-Net derivatives focusing on lightweight and attention-based mechanisms.

  - **SA-U-Lite**: U-Lite with spatial attention
  - **SDA-U-Lite**: Spatial attention + DropBlock
  - **U-Lite** / **UNet18**: Lightweight base models
  - **W-Lite** / **W-Net**: Cascaded W-shaped variants

### W-Net Architecture

<p align="center">
  <img src="doc/W-net.png" alt="W-Net Architecture Diagram" width="800">
</p>

W-Net consists of two cascaded U-shaped networks. The first U-Net acts as a coarse segmenter, passing feature maps to the second U-Net for refinement. It utilizes standard internal skip connections within each block, alongside a long "Cross-Network Skip" connection that directly links the input image features to the final output block of the second network to preserve spatial details.

## ğŸ“ Results Layout

After training, results are organized as:

```
results/
â””â”€â”€ <DATASET>/
    â””â”€â”€ <EPOCHS>/              # e.g., 100_epochs
        â”œâ”€â”€ models/            # best_<Model>.pth (Full Checkpoint)
        â”œâ”€â”€ train_hist/        # CSV + JSON summaries
        â”œâ”€â”€ performance.csv    # Tabular comparison across models
        â””â”€â”€ performance.json   # Same metrics in JSON
```

## âš™ï¸ Configuration

Edit `config.py` to manage datasets paths, register new model architectures in `MODEL_REGISTRY`, or tune `TRAINING_CONFIG` (batch size, learning rate, etc.).

-----

Happy Benchmarking\! ğŸš€